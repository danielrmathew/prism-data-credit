import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix, 
    roc_curve, roc_auc_score, auc, RocCurveDisplay
)
from sklearn.preprocessing import label_binarize
from .predict_llm import predict_fasttext
from pathlib import Path

def make_confusion_matrix(y, preds, model_type, train=True):
    """
    Generates and displays a confusion matrix with precision scores.

    Args:
        y (pd.Series): True labels.
        preds (numpy.ndarray): Predicted labels.

    Returns:
        None: Displays a heatmap of the confusion matrix.
    """
    conf_matrix = confusion_matrix(y, preds, labels=y.unique(), normalize='pred')
    classes = y.value_counts().sort_values(ascending=False).index
    conf_matrix_df = pd.DataFrame(conf_matrix, index=classes, columns=classes)

    fig, ax1 = plt.subplots(1, 1, figsize=(7, 7))
    
    sns.heatmap(conf_matrix_df, annot=True, fmt=".3f", cmap="YlGnBu", cbar=True, 
                linewidths=0.5, linecolor='gray', square=True, annot_kws={"size": 8}, ax=ax1)
    title = f'{model_type} - ' + ('Train Precision' if train else 'Test Precision')
    ax1.set_title(title)
    ax1.set_xlabel('Predicted Labels')
    ax1.set_ylabel('True Labels')

    file_title = 'train' if train else 'test'
    plt.savefig(f'result/{model_type}/{model_type}_{file_title}_confusion_matrix.png', bbox_inches='tight')

    plt.show()


def make_classification_report(y_true, y_obs, output_dict=True, zero_div=1.0):
    """
    Generate a classification report summarizing the main classification metrics.

    Parameters:
    ----------
    y_true : array-like
        Ground truth (true labels) for the dataset.
    y_obs : array-like
        Predicted labels or probabilities generated by the classifier.
    output_dict : bool, optional (default=True)
        If True, returns the classification report as a dictionary.
        If False, returns the report as a string.
    zero_div : float or str, optional (default=1.0)
        Sets the value to return when a division by zero occurs (e.g., precision/recall for classes with no samples).
        Can be set to a numeric value (e.g., 0 or 1) or the string "warn" to issue a warning.

    Returns:
    -------
    report : dict or str
        The classification report as a dictionary if `output_dict=True`, or as a string otherwise.
        The report includes metrics such as precision, recall, F1-score, and support for each class.

    """
        
    report = classification_report(y_true, y_obs, output_dict=output_dict, zero_division=zero_div)
    accuracy = report.pop('accuracy', None)
    report_df = pd.DataFrame.from_dict(report, orient='index').reset_index().rename(columns={'index': 'class'})
    report_df.loc['OVERALL_ACCURACY'] = pd.Series({
        'precision': accuracy,
        'recall': accuracy,
        'f1-score': accuracy,
        'support': accuracy
    })
    report_df = report_df.round(3)
    
    return report_df


def make_classification_report_csv(y_true, y_obs, model_type, train=True):
    report_df = make_classification_report(y_true, y_obs)    
    report_fp = Path(f'result/{model_type}_train_metrics.csv') if train else Path(f'result/{model_type}/{model_type}_test_metrics.csv')
    report_df.to_csv(report_fp)
    

def roc_score_curve(y, preds_proba, model_type, train=True):
    classes = sorted(pd.Series(y).unique())
    y_binarized = label_binarize(y, classes=classes)
    n_classes = y_binarized.shape[1]

    # Create a figure for the plot
    plt.figure(figsize=(7,7))
    
    # Generate and overlay ROC curves for each class
    for class_id in range(n_classes):
        RocCurveDisplay.from_predictions(
            y_binarized[:, class_id],
            preds_proba[:, class_id],
            name=f"Class {classes[class_id]} vs the rest",
            ax=plt.gca(),  # Use the same axes for all plots
        )
    RocCurveDisplay.from_predictions(
        y_binarized.ravel(),
        preds_proba.ravel(),
        name="Micro-Average OvR",
        plot_chance_level=True,
        ax=plt.gca()
    )
    
    # Set plot labels and title
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title(f"{model_type}: Multi-Class ROC Curve (Train Set)") if train else plt.title(f"{model_type}: Multi-Class ROC Curve (Test Set)")
    plt.grid()
    plt.legend(fontsize=7, loc="lower right")
    
    # Show the plot
    plt.savefig(f'result/{model_type}/{model_type}_train_roc_auc_curve.png') if train else plt.savefig(f'result/{model_type}/{model_type}_test_roc_auc_curve.png') 
    plt.show()


#######################################
###### FASTTEXT Evaluation Below ######
#######################################

def fasttext_data_prep(fp):
    data = []
    labels = []
    unique_labels = set()
    
    with open(fp, 'r') as f:  # Adjust the test file path
        for line in f:
            line_sp = line.strip().split(' ')
            text, label = ' '.join(line_sp[:-1]), line_sp[-1]
    
            data.append(text)
            labels.append(label)
            unique_labels.add(label)

    return data, labels, unique_labels

def roc_score_curve_fasttext(data, labels, unique_labels, model, dataset_type="test"):
    # Ensure consistent ordering and remove prefixes
    unique_labels = sorted([label.replace('__label__', '') for label in unique_labels])
    labels = [label.replace('__label__', '') for label in labels]
    
    label_to_idx = {label: i for i, label in enumerate(unique_labels)}
    true_label_indices = np.array([label_to_idx[label] for label in labels])
    
    # Predicted probabilities array
    predicted_probs = np.zeros((len(data), len(unique_labels)))
    for i, text in enumerate(data):
        predicted_labels, probs = model.predict(text, k=len(unique_labels))
        predicted_labels = [label.replace('__label__', '') for label in predicted_labels]

        for label, prob in zip(predicted_labels, probs):
            if label in label_to_idx:
                predicted_probs[i, label_to_idx[label]] = prob
    
    # One-hot encode true labels
    true_label_matrix = np.zeros_like(predicted_probs)
    true_label_matrix[np.arange(len(data)), true_label_indices] = 1
    
    # ROC-AUC computation
    fpr = {}
    tpr = {}
    roc_auc = {}
    
    for label in unique_labels:
        label_idx = label_to_idx[label]
        if np.sum(true_label_matrix[:, label_idx]) == 0:
            print(f"Warning: No true instances for label '{label}'. Assigning AUC = 0.0.")
            fpr[label], tpr[label], roc_auc[label] = [0], [0], 0.0
            continue
        
        fpr[label], tpr[label], _ = roc_curve(true_label_matrix[:, label_idx], predicted_probs[:, label_idx])
        roc_auc[label] = auc(fpr[label], tpr[label])
    
    # Macro and Micro AUC scores
    macro_roc_auc = roc_auc_score(true_label_matrix, predicted_probs, multi_class="ovr", average="macro")
    micro_roc_auc = roc_auc_score(true_label_matrix, predicted_probs, multi_class="ovr", average="micro")
    roc_auc["micro"] = micro_roc_auc
    roc_auc["macro"] = macro_roc_auc


    plt.figure(figsize=(7,7))
    for label in unique_labels:
        if label in fpr and label in roc_auc:
            plt.plot(fpr[label], tpr[label], lw=2, label=f'Class {label} (AUC = {roc_auc[label]:.2f})')
    
    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
    plt.title(f'fastText: Multi-Class ROC Curve ({dataset_type.capitalize()} Set)')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend(fontsize=7, loc='lower right')
    plt.grid()
    plt.savefig(f'result/{model_type}/fasttext_{dataset_type}_roc_auc_curve.png')  # Save as distinct file
    plt.show()
    
    return fpr, tpr, roc_auc


def output_metrics_fasttext(train_fp, test_fp, model): # just feed model the filepaths and trained model -- everything should run from here

    train_data, train_labels, _ = fasttext_data_prep(train_fp)
    test_data, test_labels, unique_labels = fasttext_data_prep(test_fp)

    # make predictions
    train_preds = predict_fasttext(model, train_data)
    test_preds = predict_fasttext(model, test_data)

    train_acc = (np.array(train_preds) == np.array(train_labels)).mean()
    test_acc = (np.array(test_preds) == np.array(test_labels)).mean()
    acc = {'Train Accuracy': train_acc, 'Test Accuracy': test_acc}

    # make confusion matrices 
    print("Creating fastText confusion matrices...")
    make_confusion_matrix(pd.Series(train_labels), train_preds, 'fasttext', train=True)
    make_confusion_matrix(pd.Series(test_labels), test_preds, 'fasttext', train=False)
    print("Saved fastText confusion matrices to result/fasttext_confusion_matrix.png") 

    print("Creating fastText classifcation reports...")
    make_classification_report_csv(train_labels, train_preds, 'fasttext', train=True)
    make_classification_report_csv(test_labels, test_preds, 'fasttext', train=False)
    print("Saved fastText classifcation reports to result/fasttext_metrics.csv") 

    # make roc curves
    print("Creating fastText ROC curves...")
    fpr_train, tpr_train, roc_auc_train = roc_score_curve_fasttext(train_data, train_labels, unique_labels, model, dataset_type="train")
    fpr_test, tpr_test, roc_auc_test = roc_score_curve_fasttext(test_data, test_labels, unique_labels, model, dataset_type="test")  
    print("Saved fastText ROC curves to result/fasttext_roc_auc_curve.png") 

    return {
        "accuracy": acc,
        "roc_auc": {
            "train": roc_auc_train,
            "test": roc_auc_test,
        },
        "fpr": {
            "train": fpr_train,
            "test": fpr_test,
        },
        "tpr": {
            "train": tpr_train,
            "test": tpr_test,
        },
    }
    
