import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, roc_auc_score
from sklearn.preprocessing import label_binarize
from .predict_llm import predict_fasttext
from pathlib import Path

def make_confusion_matrix(y, preds, model_type, train=True):
    """
    Generates and displays a confusion matrix with precision scores.

    Args:
        y (pd.Series): True labels.
        preds (numpy.ndarray): Predicted labels.

    Returns:
        None: Displays a heatmap of the confusion matrix.
    """
    conf_matrix = confusion_matrix(y, preds, labels=y.unique(), normalize='pred')
    classes = y.value_counts().sort_values(ascending=False).index
    conf_matrix_df = pd.DataFrame(conf_matrix, index=classes, columns=classes)

    fig, ax1 = plt.subplots(1, 1, figsize=(7, 7))
    
    sns.heatmap(conf_matrix_df, annot=True, fmt=".3f", cmap="YlGnBu", cbar=True, 
                linewidths=0.5, linecolor='gray', square=True, annot_kws={"size": 8}, ax=ax1)
    ax1.set_title('Precision')
    ax1.set_xlabel('Predicted Labels')
    ax1.set_ylabel('True Labels')

    title = 'train' if train else 'test'
    plt.savefig(f'result/{model_type}_{title}_confusion_matrix.png',)

    plt.show()


def make_classification_report(y_true, y_obs, output_dict=True, zero_div=1.0):
    """
    Generate a classification report summarizing the main classification metrics.

    Parameters:
    ----------
    y_true : array-like
        Ground truth (true labels) for the dataset.
    y_obs : array-like
        Predicted labels or probabilities generated by the classifier.
    output_dict : bool, optional (default=True)
        If True, returns the classification report as a dictionary.
        If False, returns the report as a string.
    zero_div : float or str, optional (default=1.0)
        Sets the value to return when a division by zero occurs (e.g., precision/recall for classes with no samples).
        Can be set to a numeric value (e.g., 0 or 1) or the string "warn" to issue a warning.

    Returns:
    -------
    report : dict or str
        The classification report as a dictionary if `output_dict=True`, or as a string otherwise.
        The report includes metrics such as precision, recall, F1-score, and support for each class.

    """
        
    report = classification_report(y_true, y_obs, output_dict=output_dict, zero_division=zero_div)
    accuracy = report.pop('accuracy', None)
    report_df = pd.DataFrame.from_dict(report, orient='index').reset_index().rename(columns={'index': 'class'})
    report_df.loc['OVERALL_ACCURACY'] = pd.Series({
        'precision': accuracy,
        'recall': accuracy,
        'f1-score': accuracy,
        'support': accuracy
    })
    
    return report_df


def make_classification_report_csv(y_true, y_obs, model_type, train=True):
    report_df = make_classification_report(y_true, y_obs)    
    report_fp = Path(f'result/{model_type}_train_metrics.csv') if train else Path(f'result/{model_type}_test_metrics.csv')
    report_df.to_csv(report_fp)


def roc_score_curve(X, y_true, y_obs, model, model_type):
    """
    Generates a report, roc_scores per category, and roc_auc_curves per category

    Args:
        X (pd.DataFrame): Model Features.
        y_true (numpy.ndarray / pd.Series): Expected values corresponding to X model features
        y_obs (numpy.ndarray / pd.Series): Predicted values corresponding to model and X model features
        model (sklearn model): model use for training and predictions

    Returns:
        fpr: false positive rates
        tpr: true positive rates
        roc_auc: ROC AUC Scores

    Saves:
        'results/* : contains the roc_auc_curve for each category for this model
        
    """
    labels = y_true.unique()

    y_test_bin = label_binarize(y_true, classes=labels)

    
    try:
        y_scores = model.predict_proba(X)
    except:
        y_scores = model.decision_function(X)


    fpr = {} # false positive rate
    tpr = {} # true positive rate
    roc_auc = {}
    
    for i, class_label in enumerate(labels):
        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_scores[:, i]) # undefined function?
        roc_auc[i] = roc_auc_score(y_test_bin[:, i], y_scores[:, i]) # undefined function?
    
    fpr["micro"], tpr["micro"], _ = roc_curve(y_test_bin.ravel(), y_scores.ravel()) # undefined function?
    roc_auc["micro"] = roc_auc_score(y_test_bin, y_scores, average="micro") # undefined function? 
    roc_auc["macro"] = roc_auc_score(y_test_bin, y_scores, average="macro") # undefined function?

    plt.figure(figsize=(10, 8))
    
    for i, class_label in enumerate(labels):
        plt.plot(fpr[i], tpr[i], label=f"Class {class_label} (AUC = {roc_auc[i]:.2f})")
    
    # Plot macro-average ROC curve
    plt.plot(fpr["micro"], tpr["micro"], label=f"Micro-average (AUC = {roc_auc['micro']:.2f})", linestyle=':', color='red')
    plt.plot([0, 1], [0, 1], 'k--', label="Random Guess")
    
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("Multi-Class ROC Curve")
    plt.legend(loc="lower right")
    plt.grid()
    plt.savefig(f'result/{model_type}_roc_auc_curve.png')
    plt.show()

    return fpr, tpr, roc_auc


#######################################
###### FASTTEXT Evaluation Below ######
#######################################

def fasttext_data_prep(fp):
    data = []
    labels = []
    unique_labels = set()
    
    with open(fp, 'r') as f:  # Adjust the test file path
        for line in f:
            line_sp = line.strip().split(' ')
            text, label = ' '.join(line_sp[:-1]), line_sp[-1]
    
            data.append(text)
            labels.append(label)
            unique_labels.add(label)

    return data, labels, unique_labels

def roc_score_curve_fasttext(data, labels, unique_labels):    
    unique_labels = sorted(unique_labels)  # Ensure consistent ordering
    label_to_idx = {label: i for i, label in enumerate(unique_labels)}
    true_label_indices = np.array([label_to_idx[label] for label in labels])
    
    predicted_probs = np.zeros((len(data), len(unique_labels)))
    for i, text in enumerate(data):
        labels, probs = model.predict(text, k=len(unique_labels))
        for label, prob in zip(labels, probs):
            predicted_probs[i, label_to_idx[label]] = prob
    
    # One-hot encode true labels
    true_label_matrix = np.zeros_like(predicted_probs)
    true_label_matrix[np.arange(len(data)), true_label_indices] = 1

    # Calculate ROC-AUC for each class
    fpr = {}
    tpr = {}
    roc_auc = {}
    
    for i, label in enumerate(unique_labels):
        fpr[label], tpr[label], _ = roc_curve(true_label_matrix[:, i], predicted_probs[:, i])
        roc_auc[label] = auc(fpr[label], tpr[label])
    
    # Calculate macro and micro ROC-AUC scores
    macro_roc_auc = roc_auc_score(true_label_matrix, predicted_probs, multi_class="ovr", average="macro") # undefined function?
    micro_roc_auc = roc_auc_score(true_label_matrix, predicted_probs, multi_class="ovr", average="micro") # undefined function?

    roc_auc  = {"micro": micro_roc_auc, 'macro': macro_roc_auc}

    # Plot ROC curve for each class
    plt.figure(figsize=(10, 8))
    for label in unique_labels:
        plt.plot(fpr[label], tpr[label], lw=2, label=f'Class {label} (AUC = {roc_auc[label]:.2f})')
    
    # Add diagonal
    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
    plt.title('Multi-Class ROC Curve')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend(loc='lower right')
    plt.grid()
    plt.savefig(f'result/fasttext_roc_auc_curve')
    plt.show()

    return fpr, tpr, roc_auc

def output_metrics_fasttext(train_fp, test_fp, model): # just feed model the filepaths and trained model -- everything should run from here

    train_data, train_labels, _ = fasttext_data_prep(train_fp)
    test_data, test_labels, unique_labels = fasttext_data_prep(test_fp)

    # make predictions
    train_preds = predict_fasttext(model, train_data)
    test_preds = predict_fasttext(model, test_data)

    train_acc = (np.array(train_preds) == np.array(train_labels)).mean()
    test_acc = (np.array(test_preds) == np.array(test_labels)).mean()
    acc = {'Train Accuracy': train_acc, 'Test Accuracy': test_acc}

    # make confusion matrices 
    print("Creating fastText confusion matrices...")
    make_confusion_matrix(pd.Series(train_labels), train_preds, 'fasttext', train=True)
    make_confusion_matrix(pd.Series(test_labels), test_preds, 'fasttext', train=False)
    print("Saved fastText confusion matrices to result/fasttext_confusion_matrix.png") 

    print("Creating fastText classifcation reports...")
    make_classification_report_csv(train_labels, train_preds, 'fasttext', train=True)
    make_classification_report_csv(test_labels, test_preds, 'fasttext', train=False)
    print("Saved fastText classifcation reports to result/fasttext_metrics.csv") 

    # make roc curves
    print("Creating fastText ROC curves...")
    fpr_train, tpr_train, roc_auc_train = roc_score_curve_fasttext(train_data, train_labels, unique_labels)
    fpr_test, tpr_test, roc_auc_test = roc_score_curve_fasttext(test_data, test_labels, unique_labels)  
    print("Saved fastText ROC curves to result/fasttext_roc_auc_curve.png") 

    return acc, fpr, tpr, roc_auc
    