import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# TODO: in output_metrics or in separate functions, add more metrics like roc-auc, classification_report stuff

def make_confusion_matrix(y, preds, model_type, train=True):
    """
    Generates and displays a confusion matrix with precision scores.

    Args:
        y (pd.Series): True labels.
        preds (numpy.ndarray): Predicted labels.

    Returns:
        None: Displays a heatmap of the confusion matrix.
    """
    conf_matrix = confusion_matrix(y, preds, labels=y.unique(), normalize='pred')
    classes = y.value_counts().sort_values(ascending=False).index
    conf_matrix_df = pd.DataFrame(conf_matrix, index=classes, columns=classes)

    fig, ax1 = plt.subplots(1, 1, figsize=(7, 7))
    
    sns.heatmap(conf_matrix_df, annot=True, fmt=".3f", cmap="YlGnBu", cbar=True, 
                linewidths=0.5, linecolor='gray', square=True, annot_kws={"size": 8}, ax=ax1)
    ax1.set_title('Precision')
    ax1.set_xlabel('Predicted Labels')
    ax1.set_ylabel('True Labels')

    title = 'train' if train else 'test'
    plt.savefig(f'../../result/{model_type}_{title}_confusion_matrix.png',)

    plt.show()


def classification_report(y_true, y_obs, output_dict=True, zero_div=1.0):
    """
    Generate a classification report summarizing the main classification metrics.

    Parameters:
    ----------
    y_true : array-like
        Ground truth (true labels) for the dataset.
    y_obs : array-like
        Predicted labels or probabilities generated by the classifier.
    output_dict : bool, optional (default=True)
        If True, returns the classification report as a dictionary.
        If False, returns the report as a string.
    zero_div : float or str, optional (default=1.0)
        Sets the value to return when a division by zero occurs (e.g., precision/recall for classes with no samples).
        Can be set to a numeric value (e.g., 0 or 1) or the string "warn" to issue a warning.

    Returns:
    -------
    report : dict or str
        The classification report as a dictionary if `output_dict=True`, or as a string otherwise.
        The report includes metrics such as precision, recall, F1-score, and support for each class.

    """
        
    report = classification_report(y_true, y_obs, output_dict=output_dict, zero_division=zero_div)

    return report


def roc_score_curve(X, y_true, y_obs, model, model_type):
    """
    Generates a report, roc_scores per category, and roc_auc_curves per category

    Args:
        X (pd.DataFrame): Model Features.
        y_true (numpy.ndarray / pd.Series): Expected values corresponding to X model features
        y_obs (numpy.ndarray / pd.Series): Predicted values corresponding to model and X model features
        model (sklearn model): model use for training and predictions

    Returns:
        fpr: false positive rates
        tpr: true positive rates
        roc_auc: ROC AUC Scores

    Saves:
        '../../results/* : contains the roc_auc_curve for each category for this model
        
    """
    labels = y_true.unique()

    y_test_bin = label_binarize(y_true, classes=labels)

    
    try:
        y_scores = log_reg.predict_proba(X_test)
    except:
        print('Exception made') # remove line in build script
        y_scores = log_reg.decision_function(X_test)


    fpr = {} # false positive rate
    tpr = {} # true positive rate
    roc_auc = {}
    
    for i, class_label in enumerate(labels):
        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_scores[:, i])
        roc_auc[i] = roc_auc_score(y_test_bin[:, i], y_scores[:, i])
    
    fpr["micro"], tpr["micro"], _ = roc_curve(y_test_bin.ravel(), y_scores.ravel())
    roc_auc["micro"] = roc_auc_score(y_test_bin, y_scores, average="micro")
    roc_auc["macro"] = roc_auc_score(y_test_bin, y_scores, average="macro")

    plt.figure(figsize=(10, 8))
    
    for i, class_label in enumerate(labels):
        plt.plot(fpr[i], tpr[i], label=f"Class {class_label} (AUC = {roc_auc[i]:.2f})")
    
    # Plot macro-average ROC curve
    plt.plot(fpr["micro"], tpr["micro"], label=f"Micro-average (AUC = {roc_auc['micro']:.2f})", linestyle=':', color='red')
    plt.plot([0, 1], [0, 1], 'k--', label="Random Guess")
    
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("Multi-Class ROC Curve")
    plt.legend(loc="lower right")
    plt.grid()
    plt.savefig(f'../../result/{model_type}_roc_auc_curve')
    plt.show()

    return fpr, tpr, roc_auc

def output_metrics(y, preds, model_type, train=True,): 
    # TODO: add other metrics here or in other functions
    make_confusion_matrix(y, preds, model_type, train=True)

    
    return {'Train Accuracy': train_acc, 'Test Accuracy': test_acc}

#######################################
###### FASTTEXT Evaluation Below ######
#######################################

def fasttext_data_prep(fp):
    data = []
    labels = []
    unique_labels = set()
    
    with open(fp, 'r') as f:  # Adjust the test file path
        for line in f:
            line_sp = line.strip().split(' ')
            text, label = ' '.join(line_sp[:-1]), line_sp[-1]
    
            data.append(text)
            labels.append(label)
            unique_labels.add(label)

    return data, labels, unique_labels

def roc_score_curve_fasttext(data, labels, unique_labels):    
    unique_labels = sorted(unique_labels)  # Ensure consistent ordering
    label_to_idx = {label: i for i, label in enumerate(unique_labels)}
    true_label_indices = np.array([label_to_idx[label] for label in labels])
    
    predicted_probs = np.zeros((len(data), len(unique_labels)))
    for i, text in enumerate(data):
        labels, probs = model.predict(text, k=len(unique_labels))
        for label, prob in zip(labels, probs):
            predicted_probs[i, label_to_idx[label]] = prob
    
    # One-hot encode true labels
    true_label_matrix = np.zeros_like(predicted_probs)
    true_label_matrix[np.arange(len(data)), true_label_indices] = 1

    # Calculate ROC-AUC for each class
    fpr = {}
    tpr = {}
    roc_auc = {}
    
    for i, label in enumerate(unique_labels):
        fpr[label], tpr[label], _ = roc_curve(true_label_matrix[:, i], predicted_probs[:, i])
        roc_auc[label] = auc(fpr[label], tpr[label])
    
    # Calculate macro and micro ROC-AUC scores
    macro_roc_auc = roc_auc_score(true_label_matrix, predicted_probs, multi_class="ovr", average="macro")
    micro_roc_auc = roc_auc_score(true_label_matrix, predicted_probs, multi_class="ovr", average="micro")

    roc_auc  = {"micro": micro_roc_auc, 'macro': macro_roc_auc}

    # Plot ROC curve for each class
    plt.figure(figsize=(10, 8))
    for label in unique_labels:
        plt.plot(fpr[label], tpr[label], lw=2, label=f'Class {label} (AUC = {roc_auc[label]:.2f})')
    
    # Add diagonal
    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
    plt.title('Multi-Class ROC Curve')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend(loc='lower right')
    plt.grid()
    plt.savefig(f'../../result/fasttext_roc_auc_curve')
    plt.show()

    return fpr, tpr, roc_auc

def output_metrics_fasttext(train_fp, test_fp, model): # just feed model the filepaths and trained model -- everything should run from here

    train_data, train_labels, _ = fasttext_data_prep(train_fp)
    test_data, test_labels, unique_labels = fasttext_data_prep(test_fp)
    
    fpr, tpr, roc_auc = roc_score_curve_fasttext(test_data, test_labels, unique_labels)

    train_preds = []
    for i in range(len(train_data)):
        p = model.predict(train_data[i])[0][0]
        train_preds.append(p)

    test_preds = []
    for i in range(len(test_data)):
        p = model.predict(test_data[i])[0][0]
        test_preds.append(p)

    train_acc = (np.array(train_preds) == np.array(train_labels)).mean()
    test_acc = (np.array(test_preds) == np.array(test_labels)).mean()
    acc = {'Train Accuracy': train_acc, 'Test Accuracy': test_acc}

    make_confusion_matrix(pd.Series(train_labels), train_preds, 'fasttext', train=True)
    make_confusion_matrix(pd.Series(test_labels), test_preds, 'fasttext', train=False)

    return acc, fpr, tpr, roc_auc
    