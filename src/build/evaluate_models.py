import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# TODO: in output_metrics or in separate functions, add more metrics like roc-auc, classification_report stuff

def make_confusion_matrix(y, preds, model_type, train=True):
    """
    Generates and displays a confusion matrix with precision scores.

    Args:
        y (pd.Series): True labels.
        preds (numpy.ndarray): Predicted labels.

    Returns:
        None: Displays a heatmap of the confusion matrix.
    """
    conf_matrix = confusion_matrix(y, preds, labels=y.unique(), normalize='pred')
    classes = y.value_counts().sort_values(ascending=False).index
    conf_matrix_df = pd.DataFrame(conf_matrix, index=classes, columns=classes)

    fig, ax1 = plt.subplots(1, 1, figsize=(7, 7))
    
    sns.heatmap(conf_matrix_df, annot=True, fmt=".3f", cmap="YlGnBu", cbar=True, 
                linewidths=0.5, linecolor='gray', square=True, annot_kws={"size": 8}, ax=ax1)
    ax1.set_title('Precision')
    ax1.set_xlabel('Predicted Labels')
    ax1.set_ylabel('True Labels')

    title = 'train' if train else 'test'
    plt.savefig(f'{model_type}_{title}_confusion_matrix.png',)

    plt.show()


def classification_report(y_true, y_obs, output_dict=True, zero_div=1.0)
    """
    Generate a classification report summarizing the main classification metrics.

    Parameters:
    ----------
    y_true : array-like
        Ground truth (true labels) for the dataset.
    y_obs : array-like
        Predicted labels or probabilities generated by the classifier.
    output_dict : bool, optional (default=True)
        If True, returns the classification report as a dictionary.
        If False, returns the report as a string.
    zero_div : float or str, optional (default=1.0)
        Sets the value to return when a division by zero occurs (e.g., precision/recall for classes with no samples).
        Can be set to a numeric value (e.g., 0 or 1) or the string "warn" to issue a warning.

    Returns:
    -------
    report : dict or str
        The classification report as a dictionary if `output_dict=True`, or as a string otherwise.
        The report includes metrics such as precision, recall, F1-score, and support for each class.

    """
        
    report = classification_report(y_true, y_obs, output_dict=output_dict, zero_division=zero_div)

    return report


def roc_score_curve(X, y_true, y_obs, model, model_type):
    """
    Generates a report, roc_scores per category, and roc_auc_curves per category

    Args:
        X (pd.DataFrame): Model Features.
        y_true (numpy.ndarray / pd.Series): Expected values corresponding to X model features
        y_obs (numpy.ndarray / pd.Series): Predicted values corresponding to model and X model features
        model (sklearn model): model use for training and predictions

    Returns:
        fpr: false positive rates
        tpr: true positive rates
        roc_auc: ROC AUC Scores

    Saves:
        '../../results/* : contains the roc_auc_curve for each category for this model
        
    """
    labels = y_true.unique()

    y_test_bin = label_binarize(y_true, classes=labels)

    
    try:
        y_scores = log_reg.predict_proba(X_test)
    except:
        print('Exception made') # remove line in build script
        y_scores = log_reg.decision_function(X_test)


    fpr = {} # false positive rate
    tpr = {} # true positive rate
    roc_auc = {}
    
    for i, class_label in enumerate(labels):
        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_scores[:, i])
        roc_auc[i] = roc_auc_score(y_test_bin[:, i], y_scores[:, i])
    
    fpr["micro"], tpr["micro"], _ = roc_curve(y_test_bin.ravel(), y_scores.ravel())
    roc_auc["micro"] = roc_auc_score(y_test_bin, y_scores, average="micro")
    roc_auc["macro"] = roc_auc_score(y_test_bin, y_scores, average="macro")

    plt.figure(figsize=(10, 8))
    
    for i, class_label in enumerate(labels):
        plt.plot(fpr[i], tpr[i], label=f"Class {class_label} (AUC = {roc_auc[i]:.2f})")
    
    # Plot macro-average ROC curve
    plt.plot(fpr["micro"], tpr["micro"], label=f"Micro-average (AUC = {roc_auc['micro']:.2f})", linestyle=':', color='red')
    plt.plot([0, 1], [0, 1], 'k--', label="Random Guess")
    
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("Multi-Class Receiver Operating Characteristic (ROC) Curve")
    plt.legend(loc="lower right")
    plt.grid()
    plt.savefig(f'../../result/{model_type}_roc_auc_curve')
    plt.show()

    return fpr, tpr, roc_auc

def output_metrics(y, preds, model_type, train=True):
    # TODO: add other metrics here or in other functions
    make_confusion_matrix(y, preds, model_type, train=train)

    
    return {'Train Accuracy': train_acc, 'Test Accuracy': test_acc}
    